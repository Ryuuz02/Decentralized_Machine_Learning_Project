# -*- coding: utf-8 -*-
"""Federated_gossip_learning_with_malicious_node_detection_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fsHE8Dd7_6PldVmfbQ3kVuwdGS6OcD2f
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset
import torchvision
from torchvision import transforms
import random
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from matplotlib.patches import Patch
from IPython.display import display
from scipy.stats import wasserstein_distance
from collections import Counter

# -------------------------------
# 1. Reproducibility
# -------------------------------
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(42)
device = "cuda" if torch.cuda.is_available() else "cpu"

# -------------------------------
# 2. Model
# -------------------------------
class SimpleImageNet(nn.Module):
    def __init__(self, dataset="MNIST"):
        super().__init__()
        self.dataset = dataset.upper()
        if self.dataset == "MNIST":
            input_dim = 28*28
        else:
            input_dim = 32*32*3
        self.net = nn.Sequential(
            nn.Flatten(),
            nn.Linear(input_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 10)
        )
    def forward(self, x):
        return self.net(x)

# -------------------------------
# 3. Malicious Behaviors
# -------------------------------
class MaliciousBehavior:
    """Different types of malicious behaviors"""

    @staticmethod
    def label_flipping(node, flip_percentage=0.3):
        """Randomly flip labels during training"""
        if random.random() < flip_percentage:
            return random.randint(0, 9)
        return None

    @staticmethod
    def weight_poisoning(node, poison_strength=0.1):
        """Add noise to model weights"""
        poisoned_weights = {}
        for key, weight in node.cached_weights.items():
            noise = torch.randn_like(weight) * poison_strength
            poisoned_weights[key] = weight + noise
        return poisoned_weights

    @staticmethod
    def gradient_attack(node, attack_type="sign_flip"):
        """Apply gradient attacks during training"""
        if attack_type == "sign_flip":
            # Flip gradient signs
            for param in node.model.parameters():
                if param.grad is not None:
                    param.grad = -param.grad * random.uniform(0.5, 2.0)
        elif attack_type == "noise_injection":
            # Add noise to gradients
            for param in node.model.parameters():
                if param.grad is not None:
                    noise = torch.randn_like(param.grad) * 0.1
                    param.grad += noise

    @staticmethod
    def data_poisoning(node, dataloader, poison_ratio=0.2):
        """Poison training data"""
        poisoned_data = []
        for x, y in dataloader:
            # Randomly modify some samples
            mask = torch.rand(len(y)) < poison_ratio
            if mask.any():
                y[mask] = torch.randint(0, 10, (mask.sum(),))
            poisoned_data.append((x, y))
        return poisoned_data

# -------------------------------
# 4. Gossip Node with Enhanced Detection
# -------------------------------
class GossipNode:
    def __init__(self, model, dataloader, node_id, test_loader, global_class_dist, is_malicious=False, malicious_type=None):
        self.model = model
        self.dataloader = dataloader
        self.id = node_id
        self.test_loader = test_loader
        self.cached_weights = None
        self.suspected_malicious = False
        self.detected_malicious_type = None
        self.is_malicious = is_malicious
        self.malicious_type = malicious_type  # Actual malicious type if node is malicious
        self.global_class_dist = global_class_dist
        self.trust_score = 1.0
        self.convergence_history = []
        self.anomaly_scores = {"dataset": 0, "weights": 0, "performance": 0}

    # Dataset anomaly detection using multiple metrics
    def dataset_anomaly_score(self):
        # Collect label distribution
        counts = torch.zeros(10)
        all_labels = []
        for _, y in self.dataloader:
            counts += torch.bincount(y, minlength=10)
            all_labels.extend(y.tolist())

        counts = counts / counts.sum()

        # 1. KL-divergence
        epsilon = 1e-8
        kl_div = (counts + epsilon) * torch.log((counts + epsilon) / (self.global_class_dist + epsilon))
        kl_score = kl_div.sum().item()

        # 2. Wasserstein distance
        wasserstein_score = wasserstein_distance(
            np.arange(10), np.arange(10),
            counts.numpy(), self.global_class_dist.numpy()
        )

        # 3. Class imbalance ratio
        max_ratio = counts.max() / (counts.min() + epsilon)
        imbalance_score = min(max_ratio / 10, 1.0)  # Normalize

        # Combine scores
        combined_score = 0.5 * kl_score + 0.3 * wasserstein_score + 0.2 * imbalance_score
        self.anomaly_scores["dataset"] = combined_score

        # Detailed diagnostics
        dataset_diagnostics = {
            "kl_divergence": kl_score,
            "wasserstein_distance": wasserstein_score,
            "imbalance_ratio": imbalance_score,
            "class_distribution": counts.tolist(),
            "most_common_class": int(counts.argmax()),
            "least_common_class": int(counts.argmin())
        }

        return combined_score, dataset_diagnostics

    # Local training with optional malicious behavior
    def local_train(self, epochs=1):
        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
        criterion = nn.CrossEntropyLoss()
        self.model.train()

        # Apply malicious behavior if node is malicious
        if self.is_malicious:
            if self.malicious_type == "label_flipping":
                flip_count = 0
                for x, y in self.dataloader:
                    x, y = x.to(device), y.to(device)
                    # Flip labels for malicious node
                    flip_mask = torch.rand(len(y)) < 0.3
                    if flip_mask.any():
                        y[flip_mask] = torch.randint(0, 10, (flip_mask.sum(),)).to(device)
                        flip_count += flip_mask.sum().item()

                    optimizer.zero_grad()
                    logits = self.model(x)
                    loss = criterion(logits, y)
                    loss.backward()
                    optimizer.step()
                # print(f"Node {self.id} (label_flipping): Flipped {flip_count} labels")

            elif self.malicious_type == "weight_poisoning":
                for x, y in self.dataloader:
                    x, y = x.to(device), y.to(device)
                    optimizer.zero_grad()
                    logits = self.model(x)
                    loss = criterion(logits, y)
                    loss.backward()
                    # Apply gradient attack
                    MaliciousBehavior.gradient_attack(self, "sign_flip")
                    optimizer.step()

            elif self.malicious_type == "data_poisoning":
                # Train normally but with poisoned data
                for x, y in self.dataloader:
                    x, y = x.to(device), y.to(device)
                    # Poison some data
                    poison_mask = torch.rand(len(y)) < 0.2
                    if poison_mask.any():
                        y[poison_mask] = torch.randint(0, 10, (poison_mask.sum(),)).to(device)

                    optimizer.zero_grad()
                    logits = self.model(x)
                    loss = criterion(logits, y)
                    loss.backward()
                    optimizer.step()

            elif self.malicious_type == "byzantine":
                # Byzantine attack: send random weights
                for param in self.model.parameters():
                    param.data = torch.randn_like(param.data) * 0.1
                optimizer.zero_grad()

        else:
            # Normal training for benign nodes
            for x, y in self.dataloader:
                x, y = x.to(device), y.to(device)
                optimizer.zero_grad()
                logits = self.model(x)
                loss = criterion(logits, y)
                loss.backward()
                optimizer.step()

        self.cached_weights = {k: v.clone() for k,v in self.model.state_dict().items()}

        # Apply weight poisoning if specified
        if self.is_malicious and self.malicious_type == "weight_poisoning":
            self.cached_weights = MaliciousBehavior.weight_poisoning(self, poison_strength=0.2)

    # Enhanced weight anomaly detection
    def detect_weight_anomaly(self, neighbors, threshold=2.0):
        if not self.cached_weights:
            return False

        my_weights = self.cached_weights
        weight_anomalies = []
        weight_diagnostics = {}

        for key in my_weights:
            layer_diffs = []
            for neighbor in neighbors:
                if neighbor.cached_weights and key in neighbor.cached_weights:
                    diff = torch.norm(my_weights[key] - neighbor.cached_weights[key]).item()
                    layer_diffs.append(diff)

            if layer_diffs:
                mean_diff = np.mean(layer_diffs)
                std_diff = np.std(layer_diffs) if len(layer_diffs) > 1 else 0.1
                my_diff = torch.norm(my_weights[key] - torch.stack([n.cached_weights[key] for n in neighbors if n.cached_weights]).mean(0)).item()

                # Multiple anomaly indicators
                z_score = (my_diff - mean_diff) / (std_diff + 1e-8)
                is_anomalous = z_score > threshold

                weight_anomalies.append(is_anomalous)
                weight_diagnostics[key] = {
                    "z_score": z_score,
                    "my_norm": torch.norm(my_weights[key]).item(),
                    "mean_neighbor_norm": np.mean([torch.norm(n.cached_weights[key]).item() for n in neighbors if n.cached_weights]),
                    "is_anomalous": is_anomalous
                }

        # Check if majority of layers are anomalous
        anomalous_layers = sum(weight_anomalies)
        total_layers = len(weight_anomalies)

        if total_layers > 0:
            anomaly_ratio = anomalous_layers / total_layers
            self.anomaly_scores["weights"] = anomaly_ratio

            if anomaly_ratio > 0.5:  # More than half of layers are anomalous
                if not self.suspected_malicious:
                    self.detected_malicious_type = "update_anomaly"
                self.suspected_malicious = True
                return True, weight_diagnostics

        return False, weight_diagnostics

    # Performance anomaly detection
    def detect_performance_anomaly(self, neighbors, threshold=0.15):
        my_acc = self.evaluate_accuracy()
        neighbor_accs = [n.evaluate_accuracy() for n in neighbors]

        if neighbor_accs:
            mean_acc = np.mean(neighbor_accs)
            std_acc = np.std(neighbor_accs) if len(neighbor_accs) > 1 else 0.05

            # Performance deviation
            perf_diff = abs(my_acc - mean_acc)
            perf_z_score = perf_diff / (std_acc + 1e-8)

            self.anomaly_scores["performance"] = perf_z_score

            # Low accuracy + high deviation is suspicious
            if my_acc < 0.5 and perf_z_score > threshold:
                if not self.suspected_malicious:
                    if self.detected_malicious_type:
                        self.detected_malicious_type = "both"
                    else:
                        self.detected_malicious_type = "performance_anomaly"
                self.suspected_malicious = True
                return True

        return False

    # Trust-based gossip aggregation
    def gossip_with(self, neighbors, alpha=0.5):
        if not neighbors:
            return

        # Filter out suspected malicious nodes
        trusted_neighbors = [n for n in neighbors if n.trust_score > 0.5 and not n.suspected_malicious]

        if not trusted_neighbors:
            # If no trusted neighbors, use all but with low weights for suspected nodes
            weight_list = []
            for n in neighbors:
                if n.cached_weights:
                    weight_list.append((n.cached_weights, n.trust_score))
        else:
            weight_list = [(n.cached_weights, n.trust_score) for n in trusted_neighbors if n.cached_weights]

        if not weight_list:
            return

        # Weighted aggregation based on trust scores
        total_trust = sum(trust for _, trust in weight_list)
        new_state = {}

        for key in weight_list[0][0]:
            weighted_sum = None
            for weights, trust in weight_list:
                if weighted_sum is None:
                    weighted_sum = weights[key] * trust
                else:
                    weighted_sum += weights[key] * trust

            # Include self with trust score
            if self.cached_weights and key in self.cached_weights:
                weighted_sum += self.cached_weights[key] * self.trust_score
                total_trust += self.trust_score

            new_state[key] = weighted_sum / total_trust

        self.model.load_state_dict(new_state)
        self.cached_weights = {k: v.clone() for k, v in self.model.state_dict().items()}

        # Update trust scores based on convergence
        self.update_trust_scores(neighbors)

    def update_trust_scores(self, neighbors):
        """Update trust scores based on similarity with neighbors"""
        if not self.cached_weights or not neighbors:
            return

        similarities = []
        for neighbor in neighbors:
            if neighbor.cached_weights:
                sim = self._compute_weight_similarity(neighbor)
                similarities.append(sim)

        if similarities:
            avg_similarity = np.mean(similarities)
            # Trust score decays if node is very different from neighbors
            self.trust_score = 0.9 * self.trust_score + 0.1 * avg_similarity
            self.convergence_history.append(avg_similarity)

    def _compute_weight_similarity(self, neighbor):
        """Compute cosine similarity between weights"""
        sims = []
        for key in self.cached_weights:
            if key in neighbor.cached_weights:
                w1 = self.cached_weights[key].flatten()
                w2 = neighbor.cached_weights[key].flatten()
                cosine_sim = torch.cosine_similarity(w1.unsqueeze(0), w2.unsqueeze(0)).item()
                sims.append(cosine_sim)
        return np.mean(sims) if sims else 0

    # Evaluate test accuracy
    def evaluate_accuracy(self):
        self.model.eval()
        correct, total = 0, 0
        with torch.no_grad():
            for x, y in self.test_loader:
                x, y = x.to(device), y.to(device)
                logits = self.model(x)
                preds = logits.argmax(1)
                correct += (preds == y).sum().item()
                total += y.size(0)
        return correct / total if total > 0 else 0

    def get_detection_summary(self):
        """Get comprehensive detection summary"""
        return {
            "node_id": self.id,
            "is_malicious": self.is_malicious,
            "malicious_type": self.malicious_type,
            "suspected_malicious": self.suspected_malicious,
            "detected_malicious_type": self.detected_malicious_type,
            "trust_score": self.trust_score,
            "anomaly_scores": self.anomaly_scores,
            "accuracy": self.evaluate_accuracy()
        }

# -------------------------------
# 5. Create nodes with malicious nodes
# -------------------------------
def make_nodes(num_nodes=10, dataset_name="MNIST", batch_size=64, malicious_count=2):
    if dataset_name=="MNIST":
        transform = transforms.Compose([transforms.ToTensor()])
        train_set = torchvision.datasets.MNIST(root="./data", train=True, download=True, transform=transform)
        test_set = torchvision.datasets.MNIST(root="./data", train=False, download=True, transform=transform)
    else:
        transform = transforms.Compose([transforms.ToTensor()])
        train_set = torchvision.datasets.CIFAR10(root="./data", train=True, download=True, transform=transform)
        test_set = torchvision.datasets.CIFAR10(root="./data", train=False, download=True, transform=transform)

    labels = torch.tensor([y for _,y in train_set])
    global_class_dist = torch.bincount(labels, minlength=10).float()
    global_class_dist /= global_class_dist.sum()

    N = len(train_set)
    chunk = N // num_nodes
    test_loader = DataLoader(test_set, batch_size=batch_size)
    nodes = []

    # Define malicious node types
    malicious_types = ["label_flipping", "weight_poisoning", "data_poisoning", "byzantine"]

    # Select malicious nodes
    malicious_indices = random.sample(range(num_nodes), malicious_count)

    for i in range(num_nodes):
        start = i*chunk
        end = (i+1)*chunk if i<num_nodes-1 else N
        subset = list(range(start, end))

        # Create natural skew for some nodes
        if i % 4 == 0:  # Skewed data distribution
            keep_classes = random.sample(range(10), 6)  # Keep only 6 classes
            subset = [idx for idx in subset if labels[idx] in keep_classes]
        elif i % 4 == 1:  # Class imbalance
            # Favor 2 classes heavily
            favored_classes = random.sample(range(10), 2)
            subset = [idx for idx in subset if labels[idx] in favored_classes or random.random() < 0.1]

        loader = DataLoader(Subset(train_set, subset), batch_size=batch_size, shuffle=True)
        model = SimpleImageNet(dataset_name).to(device)

        # Assign malicious behavior
        is_malicious = i in malicious_indices
        malicious_type = random.choice(malicious_types) if is_malicious else None

        node = GossipNode(model, loader, i, test_loader, global_class_dist,
                         is_malicious=is_malicious, malicious_type=malicious_type)
        nodes.append(node)

    print(f"\nCreated {num_nodes} nodes:")
    print(f"- Benign nodes: {num_nodes - malicious_count}")
    print(f"- Malicious nodes: {malicious_count}")
    for i in malicious_indices:
        print(f"  Node {i}: {nodes[i].malicious_type}")

    return nodes

# -------------------------------
# 6. Small-world topology
# -------------------------------
def build_small_world_topology(nodes, k=4, p=0.1):
    num_nodes = len(nodes)
    G = nx.watts_strogatz_graph(num_nodes, k, p)
    neighbors = {i:list(G.neighbors(i)) for i in range(num_nodes)}

    # Color nodes based on malicious status
    node_colors = []
    for i in range(num_nodes):
        if nodes[i].is_malicious:
            node_colors.append('red')
        elif nodes[i].suspected_malicious:
            node_colors.append('orange')
        else:
            node_colors.append('skyblue')

    plt.figure(figsize=(8, 8))
    pos = nx.spring_layout(G, seed=42)
    nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=600)
    nx.draw_networkx_edges(G, pos, alpha=0.5)
    nx.draw_networkx_labels(G, pos)

    # Create legend
    legend_elements = [
        Patch(facecolor='skyblue', label='Benign'),
        Patch(facecolor='orange', label='Suspected Malicious'),
        Patch(facecolor='red', label='Malicious (Actual)')
    ]
    plt.legend(handles=legend_elements, loc='upper right')
    plt.title("Network Topology with Malicious Nodes")
    plt.axis('off')
    display(plt.gcf())

    return neighbors

# -------------------------------
# 7. Run gossip simulation with enhanced detection
# -------------------------------
def run_gossip_simulation(nodes, rounds=10, alpha=0.5, k=4, p=0.1):
    neighbors_map = build_small_world_topology(nodes, k, p)
    accuracy_history = [[] for _ in nodes]
    trust_history = [[] for _ in nodes]
    detection_history = []

    # Type mapping for visualization
    type_to_val = {
        "benign": 0.5,
        "data_anomaly": 1.5,
        "update_anomaly": 2.5,
        "performance_anomaly": 3.5,
        "both": 4.5,
        "malicious_actual": 5.5
    }

    color_map = {
        "update_anomaly": "red",
        "data_anomaly": "green",
        "performance_anomaly": "purple",
        "both": "orange",
        "benign": "skyblue",
        "malicious_actual": "darkred"
    }

    num_nodes = len(nodes)

    print("\n" + "="*60)
    print("STARTING GOSSIP SIMULATION")
    print("="*60)

    for r in range(rounds):
        print(f"\n{'='*40}")
        print(f"ROUND {r+1}/{rounds}")
        print(f"{'='*40}")

        # 1. Local training
        print("\n[1/4] Local training...")
        for node in nodes:
            node.local_train(epochs=2)

        # 2. Dataset anomaly detection
        print("\n[2/4] Dataset anomaly detection...")
        dataset_scores = []
        dataset_diagnostics = []

        for node in nodes:
            score, diag = node.dataset_anomaly_score()
            dataset_scores.append(score)
            dataset_diagnostics.append(diag)

        mean_score = np.mean(dataset_scores)
        std_score = np.std(dataset_scores)

        for i, (node, score) in enumerate(zip(nodes, dataset_scores)):
            if score > mean_score + 2 * std_score:
                node.suspected_malicious = True
                if not node.detected_malicious_type:
                    node.detected_malicious_type = "data_anomaly"
                elif node.detected_malicious_type != "data_anomaly":
                    node.detected_malicious_type = "both"

                # Print detailed diagnostics for suspicious nodes
                print(f"\n⚠️  Node {node.id} detected with dataset anomaly:")
                print(f"   Score: {score:.4f} (mean: {mean_score:.4f}, std: {std_score:.4f})")
                print(f"   Class distribution: {dataset_diagnostics[i]['class_distribution']}")
                print(f"   Most common class: {dataset_diagnostics[i]['most_common_class']}")
                print(f"   KL divergence: {dataset_diagnostics[i]['kl_divergence']:.4f}")

        # 3. Weight and performance anomaly detection
        print("\n[3/4] Weight and performance anomaly detection...")
        if r > 0:
            for i, node in enumerate(nodes):
                neighbors = [nodes[n] for n in neighbors_map[node.id]]

                # Weight anomaly detection
                weight_anomaly, weight_diag = node.detect_weight_anomaly(neighbors, threshold=2.5)

                # Performance anomaly detection
                perf_anomaly = node.detect_performance_anomaly(neighbors, threshold=0.2)

                if weight_anomaly:
                    print(f"\n⚠️  Node {node.id} detected with weight anomaly")
                    # Show top anomalous layers
                    anomalous_layers = [(k, v['z_score']) for k, v in weight_diag.items() if v['z_score'] > 2.5]
                    if anomalous_layers:
                        print(f"   Most anomalous layers:")
                        for layer, z_score in sorted(anomalous_layers, key=lambda x: x[1], reverse=True)[:3]:
                            print(f"     {layer}: z-score = {z_score:.2f}")

                if perf_anomaly:
                    print(f"\n⚠️  Node {node.id} detected with performance anomaly")
                    acc = node.evaluate_accuracy()
                    neighbor_accs = [n.evaluate_accuracy() for n in neighbors]
                    print(f"   Accuracy: {acc:.4f}, Neighbor avg: {np.mean(neighbor_accs):.4f}")

        # 4. Gossip aggregation
        print("\n[4/4] Gossip aggregation...")
        for node in nodes:
            neighbors = [nodes[n] for n in neighbors_map[node.id]]
            node.gossip_with(neighbors, alpha)

        # Record statistics
        round_detections = []
        for i, node in enumerate(nodes):
            acc = node.evaluate_accuracy()
            accuracy_history[i].append(acc)
            trust_history[i].append(node.trust_score)

            # Determine detection status
            if node.is_malicious:
                status = "malicious_actual"
            elif node.suspected_malicious:
                status = node.detected_malicious_type or "update_anomaly"
            else:
                status = "benign"

            round_detections.append(status)

            # Print node status
            if r == rounds-1 or r % 2 == 0:  # Print every 2 rounds or last round
                status_str = "MALICIOUS" if node.suspected_malicious or node.is_malicious else "BENIGN"
                actual_str = f" (Actual: {node.malicious_type})" if node.is_malicious else ""
                print(f"Node {node.id}: {status_str}{actual_str}, Acc: {acc:.4f}, Trust: {node.trust_score:.3f}")

        detection_history.append(round_detections)

    # ========== VISUALIZATIONS ==========

    # 1. Accuracy plot
    plt.figure(figsize=(12, 5))
    for i, accs in enumerate(accuracy_history):
        line_style = '--' if nodes[i].is_malicious else '-'
        line_color = 'red' if nodes[i].is_malicious else None
        plt.plot(range(1, len(accs)+1), accs, linestyle=line_style, color=line_color,
                label=f"Node {i}{' (M)' if nodes[i].is_malicious else ''}")

    plt.xlabel("Round")
    plt.ylabel("Accuracy")
    plt.title(f"{nodes[0].model.dataset} Gossip Learning Accuracy (Malicious nodes dashed)")
    plt.grid(True, alpha=0.3)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    display(plt.gcf())

    # 2. Trust score plot
    plt.figure(figsize=(12, 5))
    for i, trusts in enumerate(trust_history):
        line_style = '--' if nodes[i].is_malicious else '-'
        line_color = 'red' if nodes[i].is_malicious else None
        plt.plot(range(1, len(trusts)+1), trusts, linestyle=line_style, color=line_color,
                label=f"Node {i}{' (M)' if nodes[i].is_malicious else ''}")

    plt.xlabel("Round")
    plt.ylabel("Trust Score")
    plt.title(f"{nodes[0].model.dataset} Node Trust Scores")
    plt.grid(True, alpha=0.3)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    display(plt.gcf())

    # 3. Detection status heatmap
    plt.figure(figsize=(14, 6))

    detection_matrix = np.zeros((num_nodes, rounds))
    for r in range(rounds):
        for i, status in enumerate(detection_history[r]):
            detection_matrix[i, r] = type_to_val[status]

    plt.imshow(detection_matrix, aspect='auto', cmap='tab20c')
    plt.colorbar(label='Detection Type', ticks=list(type_to_val.values()))

    # Add node labels
    for i in range(num_nodes):
        for r in range(rounds):
            plt.text(r, i, f"{i}", ha='center', va='center',
                    color='white' if detection_matrix[i, r] > 3 else 'black',
                    fontsize=8, fontweight='bold')

    plt.xlabel("Round")
    plt.ylabel("Node ID")
    plt.title(f"{nodes[0].model.dataset} Malicious Node Detection Timeline")
    plt.yticks(range(num_nodes), [f"Node {i}{' (M)' if nodes[i].is_malicious else ''}" for i in range(num_nodes)])
    plt.xticks(range(rounds), [f"R{r+1}" for r in range(rounds)])
    plt.tight_layout()
    display(plt.gcf())

    # 4. Final detection summary
    print("\n" + "="*60)
    print("FINAL DETECTION SUMMARY")
    print("="*60)

    true_positives = 0
    false_positives = 0
    false_negatives = 0
    true_negatives = 0

    for node in nodes:
        summary = node.get_detection_summary()
        actual = "MALICIOUS" if summary["is_malicious"] else "BENIGN"
        detected = "MALICIOUS" if summary["suspected_malicious"] else "BENIGN"

        # Update confusion matrix
        if summary["is_malicious"] and summary["suspected_malicious"]:
            true_positives += 1
        elif not summary["is_malicious"] and summary["suspected_malicious"]:
            false_positives += 1
        elif summary["is_malicious"] and not summary["suspected_malicious"]:
            false_negatives += 1
        else:
            true_negatives += 1

        print(f"\nNode {node.id}:")
        print(f"  Actual: {actual} ({summary['malicious_type'] if summary['is_malicious'] else 'benign'})")
        print(f"  Detected: {detected} ({summary['detected_malicious_type'] or 'benign'})")
        print(f"  Accuracy: {summary['accuracy']:.4f}")
        print(f"  Trust Score: {summary['trust_score']:.3f}")
        print(f"  Anomaly Scores: Dataset={summary['anomaly_scores']['dataset']:.3f}, "
              f"Weights={summary['anomaly_scores']['weights']:.3f}, "
              f"Performance={summary['anomaly_scores']['performance']:.3f}")

    # Calculate metrics
    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0
    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    accuracy = (true_positives + true_negatives) / len(nodes)

    print("\n" + "="*60)
    print("DETECTION PERFORMANCE METRICS")
    print("="*60)
    print(f"True Positives: {true_positives}")
    print(f"False Positives: {false_positives}")
    print(f"False Negatives: {false_negatives}")
    print(f"True Negatives: {true_negatives}")
    print(f"Precision: {precision:.3f}")
    print(f"Recall: {recall:.3f}")
    print(f"F1-Score: {f1_score:.3f}")
    print(f"Overall Accuracy: {accuracy:.3f}")
    print("="*60)

# -------------------------------
# 8. Run simulations
# -------------------------------
num_nodes = 12
malicious_count = 3

print("\n" + "="*60)
print("MNIST GOSSIP SYSTEM WITH MALICIOUS NODE DETECTION")
print("="*60)
mnist_nodes = make_nodes(num_nodes, "MNIST", malicious_count=malicious_count)
run_gossip_simulation(mnist_nodes, rounds=8)

print("\n" + "="*60)
print("CIFAR10 GOSSIP SYSTEM WITH MALICIOUS NODE DETECTION")
print("="*60)
cifar_nodes = make_nodes(num_nodes, "CIFAR10", malicious_count=malicious_count)
run_gossip_simulation(cifar_nodes, rounds=8)